# ğŸ—£ï¸ Multi-LLM Group Chat Discussion

> What happens when 5 different AI models discuss the same topic?

Simulate a **human-like group chat discussion** using **5 independent Large Language Models** to analyze any topic, question, or document. Each LLM has a distinct thinking style, creating diverse perspectives that surface insights no single model would find alone.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![LLMs](https://img.shields.io/badge/LLMs-5%20Models-purple.svg)

## âœ¨ What It Does

Think of it as **five friends with different thinking styles discussing something in a group chat**:

- ğŸ§  **Logical Thinker** - Structured, fact-focused, breaks things down
- ğŸ¨ **Creative Thinker** - Imaginative, speculative, alternative viewpoints  
- ğŸ¤” **Skeptical Thinker** - Questions assumptions, plays devil's advocate
- ğŸ”§ **Practical Thinker** - Real-world feasibility, cost, risks
- âš–ï¸ **Synthesizer** - Observes and bridges different viewpoints

## ğŸ¯ Use Cases

- Exploring complex decisions from multiple angles
- Stress-testing ideas before presenting them
- Finding blind spots in your thinking
- Research and analysis with built-in devil's advocate
- Brainstorming with diverse AI perspectives

## ğŸš€ Quick Start

### 1. Clone the repo

```bash
git clone https://github.com/JeetInTech/Multi-LLM-Discussion.git
cd Multi-LLM-Discussion
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up your API keys

```bash
# Copy the example env file
cp .env.example .env

# Edit .env and add your Groq API key (free)
# Get one at: https://console.groq.com/keys
```

### 4. Install Ollama (for local models)

Download from [ollama.com](https://ollama.com/download), then:

```bash
ollama pull llama3.2
ollama pull mistral
ollama pull phi3
```

### 5. Run a discussion

```bash
python main.py "Should AI be regulated?"
```

## ğŸ”§ Configuration

The system uses **5 different models** for true diversity:

| Persona | Provider | Model |
|---------|----------|-------|
| ğŸ§  Logical | Ollama (local) | `llama3.2` |
| ğŸ¤” Skeptical | Ollama (local) | `mistral` |
| âš–ï¸ Synthesizer | Ollama (local) | `phi3` |
| ğŸ¨ Creative | Groq (cloud) | `llama-3.3-70b-versatile` |
| ğŸ”§ Practical | Groq (cloud) | `qwen/qwen3-32b` |

### All Free LLM Options

| Provider | Cost | Setup |
|----------|------|-------|
| **Ollama** | 100% Free | Runs locally on your GPU |
| **Groq** | Free tier | [Get API key](https://console.groq.com/keys) |
| **Google Gemini** | Free tier | [Get API key](https://makersuite.google.com/app/apikey) |
| **HuggingFace** | Free tier | [Get API key](https://huggingface.co/settings/tokens) |

## ğŸ“ Project Structure

```
Multi-LLM-Discussion/
â”œâ”€â”€ main.py           # CLI entry point
â”œâ”€â”€ discussion.py     # Core discussion engine
â”œâ”€â”€ personas.py       # 5 persona definitions
â”œâ”€â”€ llm_clients.py    # API clients (Ollama, Groq, etc.)
â”œâ”€â”€ config.py         # Configuration settings
â”œâ”€â”€ demo_offline.py   # Demo without API keys
â”œâ”€â”€ .env.example      # Template for API keys
â”œâ”€â”€ requirements.txt  # Python dependencies
â””â”€â”€ README.md
```

## ğŸ’» Command Line Options

```bash
python main.py [topic] [options]

Options:
  --file, -f FILE    Read topic from a file
  --rounds, -r N     Number of discussion rounds (default: 3)
  --no-synth         Disable the Neutral Synthesizer
```

## ğŸ“‹ Example Output

```
[User]
Should remote work become the default?

â”€â”€â”€ Round 1 â”€â”€â”€

[ğŸ§  Logical Thinker]
Let's look at the data. Studies show remote workers are often 
more productive, with less commute stress...

[ğŸ¨ Creative Thinker]
What if the office isn't about work at all? Maybe it's about 
spontaneous collisions of ideas...

[ğŸ¤” Skeptical Thinker]
But those productivity studies mostly come from self-reporting. 
How do we actually measure creativity remotely?...

[ğŸ”§ Practical Thinker]
The real question is: can you maintain culture and onboard 
new people effectively?...
```

## ğŸ”’ Safety Design

This is **NOT an autonomous agent framework**. It's a conversation simulator.

- âŒ No tool usage
- âŒ No memory persistence  
- âŒ No self-reflection loops
- âŒ No planning or execution
- âœ… Each LLM responds only to visible chat history
- âœ… Pure text generation simulation

## ğŸ¤ Contributing

Contributions welcome! Ideas:
- Additional personas
- New LLM providers
- Export formats (Markdown, HTML, JSON)
- Web interface

## ğŸ“„ License

MIT License - Free to use for any purpose. See [LICENSE](LICENSE) file.

## ğŸ‘¤ Author

**JeetInTech** - [GitHub](https://github.com/JeetInTech)

---

â­ Star this repo if you find it useful!